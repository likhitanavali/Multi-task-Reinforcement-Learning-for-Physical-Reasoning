import torch
import torch.nn as nn

from src.vtrace.VTrace import VTrace

from typing import Tuple, Dict

try:
    from typing_extensions import Final
except:
    from torch.jit import Final


class Impala(nn.Module):
    """Network that formalises the methods describe in the IMPALA paper"""

    __constants__ = ["entropy_coef", "value_coef", "discount_factor"]

    def __init__(
        self,
        sequence_length,
        entropy_coef,
        value_coef,
        discount_factor,
        model,
        rho=1.0,
        cis=1.0,
        device="cuda",
    ):
        super(Impala, self).__init__()

        self.device = device

        self.model = model

        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        self.discount_factor = discount_factor

        self.sequence_length = sequence_length

        # V-trace should be computed on CPU (more efficient)
        # self.vtrace = torch.jit.script(
        self.vtrace =     VTrace(
                discount_factor=discount_factor,
                rho=rho,
                cis=cis,
                sequence_length=self.sequence_length,
            # )
        ).to("cpu")

    def update_network(self, target_model):
        """Updates the weights with the encapsulated network"""
        self.model.load_state_dict(target_model.state_dict())

    def get_model_state_dict(self):
        """Only saves the weights of the encapsulated model"""
        return self.model.state_dict()

    def forward(
        self,
        obs: torch.Tensor,
        behaviour_actions: torch.Tensor,
        reset_mask: torch.Tensor,
        lstm_hxs: Tuple[torch.Tensor, torch.Tensor],
        rewards: torch.Tensor,
        behaviour_log_probs: torch.Tensor,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Parameters
        ----------
        obs : torch.tensor
            Observations recorded by the agents
            float32 tensor of shape [T+1, B, ...]
        behaviour_actions : torch.tensor
            Actions generated with the behaviour policy
            float32 tensor of shape [T+1, B, ...]
        reset_mask : torch.tensor
            Whether or not the episode is finished (start a new episode)
            float32 tensor of shape [T+1, B, ...]
        lstm_hxs : torch.tensor
            Hidden states of LSTM
        rewards : torch.tensor
            Rewards collected by the agent
        behaviour_log_probs : torch.tensor
            Log probs generated by the behaviour policy

        Returns
        -------
        loss : torch.tensor
            computed loss
        detached_loss : 
            List of the different losses
        """
        # Forward pass of the model
        target_log_probs, target_entropy, target_value = self.model(
            obs=obs,
            lstm_hxs=lstm_hxs,
            mask=reset_mask,
            behaviour_actions=behaviour_actions,
        )

        # VTrace
        v_targets, rhos = self.vtrace(
            target_value=target_value.cpu(),  # Contains the bootstrapping
            target_log_policy=target_log_probs[:-1].cpu(),  # We remove bootstrap
            rewards=rewards.cpu(),
            behaviour_log_policy=behaviour_log_probs.cpu(),
        )
        v_targets = v_targets.to(self.device)
        rhos = rhos.to(self.device)

        # Losses computation

        # Value loss = l2 target loss -> (v_s - V_w(x_s))**2
        loss_value = (v_targets - target_value).pow_(
            2
        )  # No need to remove bootstrap as diff equals zero
        loss_value = loss_value.sum()

        # Policy loss -> - rho * advantage * log_policy & entropy bonus sum(policy*log_policy)
        # We detach the advantage because we don't compute
        # A = reward + gamma * V_{t+1} - V_t
        # L = - log_prob * A
        # The advantage function reduces variance
        advantage = rewards + self.discount_factor * v_targets[1:] - target_value[:-1]
        loss_policy = -rhos * target_log_probs[:-1] * advantage.detach()
        loss_policy = loss_policy.sum()

        # Adding the entropy bonus (much like A3C for instance)
        # The entropy is like a measure of the disorder
        entropy = target_entropy[:-1].sum()

        # Summing all the losses together
        loss = loss_policy + self.value_coef * loss_value - self.entropy_coef * entropy

        # These are only used for the statistics
        detached_losses = {
            "policy": loss_policy.detach().cpu(),
            "value": loss_value.detach().cpu(),
            "entropy": entropy.detach().cpu(),
        }

        return loss, detached_losses

    # @torch.jit.export
    def loss(
        self,
        obs: torch.Tensor,
        behaviour_actions: torch.Tensor,
        reset_mask: torch.Tensor,
        lstm_hxs: Tuple[torch.Tensor, torch.Tensor],
        rewards: torch.Tensor,
        behaviour_log_probs: torch.Tensor,
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        return self.forward(
            obs=obs,
            behaviour_actions=behaviour_actions,
            reset_mask=reset_mask,
            lstm_hxs=lstm_hxs,
            rewards=rewards,
            behaviour_log_probs=behaviour_log_probs,
        )

    # @torch.jit.export
    def act(self, obs: torch.Tensor, lstm_hxs: Tuple[torch.Tensor, torch.Tensor]):
        """
        Parameters
        ----------
        obs : torch.tensor
            shape (batch, c, h, w)
        lstm_hxs : torch.tensor
            shape (1, batch, hidden)

        Returns
        -------
        action: torch.Tensor
        log_prob : torch.Tensor
        lstm_hxs : torch.Tensor
        """
        action, log_prob, lstm_hxs = self.model.act(obs, lstm_hxs)
        return action, log_prob, lstm_hxs
